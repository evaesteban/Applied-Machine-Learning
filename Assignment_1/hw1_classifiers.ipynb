{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HW1 - Classifiers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Digit Recognizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pyplot import imshow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read training and testing data\n",
    "training_data=pd.read_csv(\"Digit_Recognition_Data/train.csv\")\n",
    "testing_data=pd.read_csv(\"Digit_Recognition_Data/test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to display the MNIST digit indicated by the label parameter\n",
    "def findDigit(label, data):\n",
    "    digitIndex = list(labels).index(label)\n",
    "    return data[digitIndex,:,:], digitIndex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels=training_data['label'].values #extract data labels\n",
    "data = training_data.drop(['label'],axis=1).values # extract data values\n",
    "data_shaped = data.reshape(-1,28,28) # shape data for display"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot one of each digit\n",
    "plt.figure(figsize=(20,20))\n",
    "for i in range(10):\n",
    "    plt.subplot(5,5,i+1)\n",
    "    digit, indx = findDigit(i, data_shaped)\n",
    "    imshow(digit)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to calculate the prior probability of the class indicated by the label parameter\n",
    "def calculatePrior(label):\n",
    "    return np.count_nonzero(labels == label)/(42000)\n",
    "\n",
    "# function to calculate the number of times an element of the class indicated by the label parameter appears\n",
    "# don't need this function if we decide to use plt.hist\n",
    "def counts(label):\n",
    "    return np.count_nonzero(labels == label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0, 9):\n",
    "    print (\"Prior probability for \", i, \": \", calculatePrior(i))\n",
    "\n",
    "# plots a histogram of the labels, density = True is for normalization\n",
    "plt.hist(labels, bins = 10, density = True)\n",
    "plt.xticks(np.arange(10), ['0','1','2', '3', '4', '5', '6', '7', '8', '9'])\n",
    "plt.title('MNIST Digits Frequency Histogram')\n",
    "plt.show()\n",
    "\n",
    "# changed this because I think they wanted a normalized historgram, not just a plot of counts\n",
    "#plt.bar(x, height= [counts(0), counts(1), counts(2), counts(3), counts(4), counts(5), counts(6), counts(7), counts(8), counts(9)]) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def euclideanDist(img1, img2):\n",
    "    return np.sqrt(np.sum((img2-img1)**2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find one of each digit\n",
    "listofDigits = [findDigit(i, data_shaped) for i in range(10)]\n",
    "\n",
    "# separate list of digits and their indices\n",
    "digits = []\n",
    "dig_indxs = []\n",
    "for digitTuple in listofDigits:\n",
    "    digits.append(digitTuple[0])\n",
    "    dig_indxs.append(digitTuple[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find nearest neighbor to each digit\n",
    "# create a list of nearest neighbors to each example digit (example digits are the same ones printed before)\n",
    "\n",
    "nearest_neighbors = []\n",
    "\n",
    "for i, digit in enumerate(digits):\n",
    "    indx = dig_indxs[i]\n",
    "    for j, img in enumerate(data_shaped):\n",
    "        # skip the image that we are finding the nearest neighbor for\n",
    "        if j == indx:\n",
    "            continue\n",
    "        # calculate distance\n",
    "        dist = euclideanDist(np.array(digit), np.array(img))\n",
    "        # update best distance\n",
    "        if j == 0:\n",
    "            best_dist = dist\n",
    "            nearest_n = img\n",
    "            best_indx = j\n",
    "        else:\n",
    "            if dist < best_dist:\n",
    "                best_dist = dist\n",
    "                nearest_n = img\n",
    "                best_indx = j\n",
    "    nearest_neighbors.append((nearest_n, best_indx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,20))\n",
    "for i, neighbor in enumerate(nearest_neighbors):\n",
    "    neighbor, indx = neighbor\n",
    "    if i != labels[indx]:\n",
    "        print('*', end = ' ')\n",
    "    print('The nearest neighbor for the image selected for', i, 'is at index', indx)\n",
    "    plt.subplot(5,5,i+1)\n",
    "    imshow(neighbor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zerosIndexes = []\n",
    "onesIndexes = []\n",
    "\n",
    "distZeros = []\n",
    "distOnes = []\n",
    "\n",
    "distImpostors = []\n",
    "\n",
    "for i, label in enumerate(labels):\n",
    "    if[labels[i] == 0]:\n",
    "        zerosIndexes.append(i)\n",
    "    elif[labels[i] == 1]:\n",
    "        onesIndexes.append(i)\n",
    "\n",
    "for i in range(len(zerosIndexes) - 2):\n",
    "    distZeros.append(euclideanDist(data_shaped[zerosIndexes[i]], data_shaped[zerosIndexes[i + 1]]))\n",
    "\n",
    "for i in range(len(onesIndexes) - 2):\n",
    "    distOnes.append(euclideanDist(data_shaped[onesIndexes[i]], data_shaped[onesIndexes[i + 1]]))\n",
    "    \n",
    "for i in range(len(zerosIndexes)):\n",
    "    for j in range(len(onesIndexes)):\n",
    "        distImpostors.append(euclideanDist(data_shaped[zerosIndexes[i]], data_shaped[onesIndexes[j]]))\n",
    "        \n",
    "#distZeros.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. The Titanic Disaster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.datasets import make_classification\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read training and testing data\n",
    "training_data_t=pd.read_csv(\"Titanic_Data/train_titanic.csv\")\n",
    "testing_data_t=pd.read_csv(\"Titanic_Data/test_titanic.csv\")\n",
    "labels_t=training_data_t['Survived'].values #extract data labels\n",
    "data_t=training_data_t.drop(['Survived'],axis=1).values # extract data values\n",
    "\n",
    "#lr = LogisticRegression()\n",
    "#lr.fit(data_t, labels_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
