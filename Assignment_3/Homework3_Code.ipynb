{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentiment Analysis for Online Reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\simir\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\simir\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\simir\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# libraries to import\n",
    "import string\n",
    "import math\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk as nltk\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "from nltk.corpus import stopwords \n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer  \n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import GaussianNB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### a) Downloading, reading and analyzing datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data in the right> format according to readme files\n",
    "yelp=pd.read_csv(\"sentiment_labelled_sentences\\yelp_labelled.txt\",delimiter=\"\\t\", names=[\"Sentence\", \"Label\"])\n",
    "imdb=pd.read_csv(\"sentiment_labelled_sentences\\imdb_labelled.txt\",delimiter=\"\\t\", names=[\"Sentence\", \"Label\"])\n",
    "amazon=pd.read_csv(\"sentiment_labelled_sentences\\labelled_amazon.txt\",delimiter=\"\\t\", names=[\"Sentence\", \"Label\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of 1s in Yelp: 500\n",
      "Number of 0s in Yelp: 500\n",
      "Number of 1s in Imdb: 386\n",
      "Number of 0s in Imdb: 362\n",
      "Number of 1s in Amazon: 500\n",
      "Number of 0s in Amazon: 500\n"
     ]
    }
   ],
   "source": [
    "# check if data is balance in all three dataframes\n",
    "\n",
    "# yelp\n",
    "ones_yelp = len(yelp[yelp['Label'] == 1])\n",
    "zeros_yelp = len(yelp[yelp['Label'] == 0])\n",
    "print('Number of 1s in Yelp:', ones_yelp)\n",
    "print('Number of 0s in Yelp:', zeros_yelp)\n",
    "\n",
    "#imdb\n",
    "ones_imdb = len(imdb[imdb['Label'] == 1])\n",
    "zeros_imdb = len(imdb[imdb['Label'] == 0])\n",
    "print('Number of 1s in Imdb:', ones_imdb)\n",
    "print('Number of 0s in Imdb:', zeros_imdb)\n",
    "\n",
    "#amazon\n",
    "ones_amazon = len(amazon[amazon['Label'] == 1])\n",
    "zeros_amazon = len(amazon[amazon['Label'] == 0])\n",
    "print('Number of 1s in Amazon:', ones_amazon)\n",
    "print('Number of 0s in Amazon:', zeros_amazon)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data in the Yelp and Amazon files is balanced because there are the same number of 1s and 0s as labels. \n",
    "The data in the Imdb file can be considered almost balancen because the number of 1s and 0s is almost the same (386 and 362, respectively). The ratio of 1s to 0s is 386/362 = 1.067."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### b) Pre-processing datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing(data):\n",
    "    stop_words = set(stopwords.words('english')) # find stop words in English language\n",
    "    lemmatizer = WordNetLemmatizer() # declare nltk lemmatizer\n",
    "\n",
    "    # iterate through every sentence and replace it by itself lemmatized, without punctuation and without stop words\n",
    "    for i in data['Sentence'].index:\n",
    "    \n",
    "        # remove punctuation\n",
    "        sentence_no_punct = ''\n",
    "        for char in (data.at[i, 'Sentence']):\n",
    "            if char not in string.punctuation:\n",
    "                sentence_no_punct = sentence_no_punct + char\n",
    "        (data.at[i, 'Sentence']) = sentence_no_punct\n",
    "\n",
    "        word_tokens = word_tokenize(data.at[i, 'Sentence'])\n",
    "        # remove stop words and lemmatize\n",
    "        word_tokens = [lemmatizer.lemmatize(word) for word in word_tokens if word not in stop_words]\n",
    "        word_tokens = [lemmatizer.lemmatize(word, 'v') for word in word_tokens]\n",
    "        word_tokens = [lemmatizer.lemmatize(word, 'a') for word in word_tokens]\n",
    "        # remove conjunction words\n",
    "        word_tokens = [word for word in word_tokens if word[-2:] != 'nt']\n",
    "        (data.at[i, 'Sentence']) = ' '.join(word_tokens)\n",
    "        \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert all letters to lower case\n",
    "yelp = yelp.applymap(lambda s:s.lower() if type(s) == str else s)\n",
    "imdb = imdb.applymap(lambda s:s.lower() if type(s) == str else s)\n",
    "amazon = amazon.applymap(lambda s:s.lower() if type(s) == str else s)\n",
    "\n",
    "# lemmatize, remove punctuation, remove stop words\n",
    "preprocessing(yelp)\n",
    "preprocessing(imdb)\n",
    "preprocessing(amazon);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this part, we decided to convert all sentences to lower case, so that the same word with some upper case letters and without them would not be detected as different words since we are using the string type which takes into account their differences. \n",
    "\n",
    "We also stripped the sentences of stop words because they do not add any meaning as the same stop words appear in many different sentences.\n",
    "\n",
    "Additionally, we removed the punctuation because it does not add any meaning to the word analysis exercise we will do in this question. \n",
    "\n",
    "We also lemmatized all the words because we are interested in knowing which class of words they belong to in order to understand the meaning of the sentence and not whether they are a noun, adjective, etc.\n",
    "\n",
    "We finally removed words with the conjunction \"n't\" because these are also meaningless words like \"isn't\" and \"didn't\". They are basically stop words but are not caught by the stop word checker because of the extra \"n't\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### c) Split training and testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the three datasets into training and testing data according to the specifications\n",
    "\n",
    "def split_data(data):\n",
    "    data.reset_index(drop=True)\n",
    "    training = (data.query('Label == 1' )).head(400).append((data.query('Label == 0' )).head(400))\n",
    "    testing = (data.query('Label == 1' )).tail(100).append((data.query('Label == 0' )).tail(100))\n",
    "    return training, testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split yelp\n",
    "training_yelp, testing_yelp = split_data(yelp)\n",
    "\n",
    "# split imdb\n",
    "training_imdb, testing_imdb = split_data(imdb)\n",
    "\n",
    "# split amazon\n",
    "training_amazon, testing_amazon = split_data(amazon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2348, 2)\n",
      "(600, 2)\n"
     ]
    }
   ],
   "source": [
    "# concatenate training and testing data for all files\n",
    "train_data = training_yelp.append(training_imdb, ignore_index=True).append(training_amazon, ignore_index=True)\n",
    "print(np.shape(train_data)) # this not exactly 2400? Because IMDB does not reach 800 data points...but why\n",
    "\n",
    "test_data = testing_yelp.append(testing_imdb, ignore_index=True).append(testing_amazon, ignore_index=True)\n",
    "print(np.shape(test_data)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### d) Bag of Words model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this question we cannot use the testing set to create the dictionary of unique words because the model needs to be created with the training set so that we can use the testing set as new data to test our model's ability to generalize. If we create the dictionary with the testing data, we are esentially using all the data as training data and would need to look for another set of new data to test the classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create set of unique words in training set\n",
    "word_dictionary = {}\n",
    "\n",
    "# iterate through every word or every sentence and store it in dictionary with count 0 (the count will be updated\n",
    "# later when we iterate through both testing and training set\n",
    "for i in train_data.index:\n",
    "    word_tokens_training = word_tokenize(train_data.at[i, 'Sentence'])\n",
    "    for word in word_tokens_training:\n",
    "        word_dictionary[word] = 0\n",
    "        \n",
    "# count the number of occurences of each word in dictionary in training set\n",
    "for i in train_data.index:\n",
    "    word_tokens_training = word_tokenize(train_data.at[i, 'Sentence'])\n",
    "    for word in word_tokens_training:\n",
    "        if word in word_dictionary:\n",
    "            word_dictionary[word] += 1\n",
    "            \n",
    "# count the number of occurences of each word in dictionary in testing set  \n",
    "for i in test_data.index:\n",
    "    word_tokens_testing = word_tokenize(test_data.at[i, 'Sentence'])\n",
    "    for word in word_tokens_testing:\n",
    "        if word in word_dictionary:\n",
    "            word_dictionary[word] += 1\n",
    "\n",
    "# create one feature vector per review\n",
    "feature_column = [] # list to store the feature vectors and add to dataframe at the end\n",
    "for i in train_data.index:\n",
    "    word_tokens_training = word_tokenize(train_data.at[i, 'Sentence'])\n",
    "    feature_vector = np.zeros(len(word_dictionary.keys())) # to store feature vector in each iteration\n",
    "    for j, dict_word in enumerate(word_dictionary.keys()):\n",
    "        if dict_word in word_tokens_training:\n",
    "            feature_vector[j] = math.log(word_dictionary[dict_word] + 1) # apply log normalization\n",
    "    feature_column.append(feature_vector) \n",
    "train_data['Vectors'] = feature_column\n",
    "\n",
    "feature_column = [] # list to store the feature vectors and add to dataframe at the end\n",
    "for i in test_data.index:\n",
    "    word_tokens_testing = word_tokenize(test_data.at[i, 'Sentence'])\n",
    "    feature_vector = np.zeros(len(word_dictionary.keys())) # to store feature vector in each iteration\n",
    "    for j, dict_word in enumerate(word_dictionary.keys()):\n",
    "        if dict_word in word_tokens_testing:\n",
    "            feature_vector[j] = math.log(word_dictionary[dict_word] + 1) # apply log normalization\n",
    "    feature_column.append(feature_vector) \n",
    "test_data['Vectors'] = feature_column"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### e) Pick postprocessing strategy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will not apply mean substraction and division by standard deviation because this only makes sense if the data more or less follows a normal distribution. Since we cannot ensure this, we will choose some other method.\n",
    "\n",
    "Also, the number of dimensions (words) of the feature vectors is significant, so choosing L1 or L2 norm might result in similar distance measures for sentences with different words, so we discarded these techniques. \n",
    "\n",
    "Log normalization can allow us to make data less skewed and focus on the relative differences rather than the absolute values, so we chose to apply this technique.\n",
    "\n",
    "We added the normalization step in the code for d) as we were constructing the feature vectors in order to reduce the number of iterations we need to go through the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### f) Sentiment prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split training and testing data into X (data) and Y (labels)\n",
    "x_train = list(train_data['Vectors']) \n",
    "y_train = list(train_data['Label'])\n",
    "\n",
    "x_test = list(test_data['Vectors']) \n",
    "y_test = list(test_data['Label'])\n",
    "\n",
    "# initialize and fit logistic regression model with training data\n",
    "lr = LogisticRegression()\n",
    "lr.fit(x_train, y_train)\n",
    "\n",
    "# predict the result for the testing data\n",
    "lr_pred = lr.predict(x_test) \n",
    "\n",
    "# calculate accuracy\n",
    "lr_acc = accuracy_score(lr_pred, y_test)\n",
    "\n",
    "# initialize and fit naive bayes model gaussian prior with training data\n",
    "gnb = GaussianNB()\n",
    "gnb.fit(x_train, y_train)\n",
    "\n",
    "# predict the results for the test set\n",
    "gnb_pred = gnb.predict(x_test) \n",
    "\n",
    "# calculate accuracy\n",
    "gnb_acc = accuracy_score(gnb_pred, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression accuracy is:  0.8466666666666667\n",
      "Naive Bayes accuracy is:  0.7916666666666666\n"
     ]
    }
   ],
   "source": [
    "print(\"Logistic Regression accuracy is: \", lr_acc)\n",
    "print(\"Naive Bayes accuracy is: \", gnb_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (g) N-gram model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ngram_dict(n):\n",
    "    # create set of unique words in training set\n",
    "    word_dict = {}\n",
    "\n",
    "    # iterate through every word or every sentence and store it in dictionary with count 0 (the count will be updated\n",
    "    # later when we iterate through both testing and training set\n",
    "    for i in train_data.index:\n",
    "        word_tokens_training = word_tokenize(train_data.at[i, 'Sentence'])\n",
    "        for j in range(len(word_tokens_training)):\n",
    "            if j+n < len(word_tokens_training):\n",
    "                words = ' '.join([word_tokens_training[k] for k in range(j,j+n)])\n",
    "                word_dict[words] = 0\n",
    "    \n",
    "    # count the number of occurences of each word in dictionary in training set\n",
    "    for i in train_data.index:\n",
    "        word_tokens_training = word_tokenize(train_data.at[i, 'Sentence'])\n",
    "        for j in range(len(word_tokens_training)):\n",
    "            if j+n < len(word_tokens_training):\n",
    "                words = ' '.join([word_tokens_training[k] for k in range(j,j+n)])\n",
    "                if words in word_dict:\n",
    "                    word_dict[words] += 1\n",
    "    # count the number of occurences of each word in dictionary in testing set  \n",
    "    for i in test_data.index:\n",
    "        word_tokens_testing = word_tokenize(test_data.at[i, 'Sentence'])\n",
    "        for j in range(len(word_tokens_testing)):\n",
    "            if j+n < len(word_tokens_testing):\n",
    "                words = ' '.join([word_tokens_testing[k] for k in range(j,j+n)])\n",
    "                if words in word_dict:\n",
    "                    word_dict[words] += 1\n",
    "    return word_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_dict = ngram_dict(2)\n",
    "\n",
    "# create one feature vector per review\n",
    "feature_column_train_ngram = [] # list to store the feature vectors and add to dataframe at the end\n",
    "for i in train_data.index:\n",
    "    word_tokens_training = word_tokenize(train_data.at[i, 'Sentence'])\n",
    "    feature_vector = np.zeros(len(word_dict.keys())) # to store feature vector in each iteration\n",
    "    for j, dict_word in enumerate(word_dict.keys()):\n",
    "        if dict_word in word_tokens_training:\n",
    "            feature_vector[j] = math.log(word_dict[dict_word] + 1) # apply log normalization\n",
    "    feature_column_train_ngram.append(feature_vector) \n",
    "train_data['Vectors_2gram'] = feature_column_train_ngram\n",
    "\n",
    "feature_column_test_ngram = [] # list to store the feature vectors and add to dataframe at the end\n",
    "for i in test_data.index:\n",
    "    word_tokens_testing = word_tokenize(test_data.at[i, 'Sentence'])\n",
    "    feature_vector = np.zeros(len(word_dict.keys())) # to store feature vector in each iteration\n",
    "    for j, dict_word in enumerate(word_dict.keys()):\n",
    "        if dict_word in word_tokens_testing:\n",
    "            feature_vector[j] = math.log(word_dict[dict_word] + 1) # apply log normalization\n",
    "    feature_column_test_ngram.append(feature_vector) \n",
    "test_data['Vectors_2gram'] = feature_column_test_ngram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\simir\\Anaconda3\\lib\\site-packages\\sklearn\\naive_bayes.py:432: RuntimeWarning: divide by zero encountered in log\n",
      "  n_ij = - 0.5 * np.sum(np.log(2. * np.pi * self.sigma_[i, :]))\n",
      "C:\\Users\\simir\\Anaconda3\\lib\\site-packages\\sklearn\\naive_bayes.py:434: RuntimeWarning: invalid value encountered in true_divide\n",
      "  (self.sigma_[i, :]), 1)\n"
     ]
    }
   ],
   "source": [
    "### THIS HAS BUGS WORKING TO FIGURE OUT ####\n",
    "# split training and testing data into X (data) and Y (labels)\n",
    "x_train_2 = list(train_data['Vectors_2gram']) \n",
    "y_train_2 = list(train_data['Label'])\n",
    "\n",
    "x_test_2 = list(test_data['Vectors_2gram']) # we still do not have vectors\n",
    "y_test_2 = list(test_data['Label'])\n",
    "\n",
    "# initialize and fit logistic regression model with training data\n",
    "lr_2 = LogisticRegression()\n",
    "lr_2.fit(x_train_2, y_train_2)\n",
    "\n",
    "# predict the result for the testing data\n",
    "lr_pred_2 = lr_2.predict(x_test_2) \n",
    "\n",
    "# calculate accuracy\n",
    "lr_acc_2 = accuracy_score(lr_pred_2, y_test_2)\n",
    "\n",
    "# initialize and fit naive bayes model gaussian prior with training data\n",
    "gnb_2 = GaussianNB()\n",
    "gnb_2.fit(x_train_2, y_train_2)\n",
    "\n",
    "# predict the results for the test set\n",
    "gnb_pred_2 = gnb_2.predict(x_test_2) \n",
    "\n",
    "# calculate accuracy\n",
    "gnb_acc_2 = accuracy_score(gnb_pred_2, y_test_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression accuracy is:  0.5\n",
      "Naive Bayes accuracy is:  0.5\n"
     ]
    }
   ],
   "source": [
    "print(\"Logistic Regression accuracy is: \", lr_acc_2)\n",
    "print(\"Naive Bayes accuracy is: \", gnb_acc_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# implement with np.svd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clustering for Text Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
