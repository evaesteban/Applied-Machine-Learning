{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentiment Analysis for Online Reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\evatr\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\evatr\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\evatr\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# libraries to import\n",
    "import string\n",
    "import math\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk as nltk\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "from nltk.corpus import stopwords \n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer  \n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import GaussianNB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### a) Downloading, reading and analyzing datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data in the right> format according to readme files\n",
    "yelp=pd.read_csv(\"sentiment_labelled_sentences\\yelp_labelled.txt\",delimiter=\"\\t\", names=[\"Sentence\", \"Label\"])\n",
    "imdb=pd.read_csv(\"sentiment_labelled_sentences\\imdb_labelled.txt\",delimiter=\"\\t\", names=[\"Sentence\", \"Label\"])\n",
    "amazon=pd.read_csv(\"sentiment_labelled_sentences\\labelled_amazon.txt\",delimiter=\"\\t\", names=[\"Sentence\", \"Label\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of 1s in Yelp: 500\n",
      "Number of 0s in Yelp: 500\n",
      "Number of 1s in Imdb: 386\n",
      "Number of 0s in Imdb: 362\n",
      "Number of 1s in Amazon: 500\n",
      "Number of 0s in Amazon: 500\n"
     ]
    }
   ],
   "source": [
    "# check if data is balance in all three dataframes\n",
    "\n",
    "# yelp\n",
    "ones_yelp = len(yelp[yelp['Label'] == 1])\n",
    "zeros_yelp = len(yelp[yelp['Label'] == 0])\n",
    "print('Number of 1s in Yelp:', ones_yelp)\n",
    "print('Number of 0s in Yelp:', zeros_yelp)\n",
    "\n",
    "#imdb\n",
    "ones_imdb = len(imdb[imdb['Label'] == 1])\n",
    "zeros_imdb = len(imdb[imdb['Label'] == 0])\n",
    "print('Number of 1s in Imdb:', ones_imdb)\n",
    "print('Number of 0s in Imdb:', zeros_imdb)\n",
    "\n",
    "#amazon\n",
    "ones_amazon = len(amazon[amazon['Label'] == 1])\n",
    "zeros_amazon = len(amazon[amazon['Label'] == 0])\n",
    "print('Number of 1s in Amazon:', ones_amazon)\n",
    "print('Number of 0s in Amazon:', zeros_amazon)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data in the Yelp and Amazon files is balanced because there are the same number of 1s and 0s as labels. \n",
    "The data in the Imdb file can be considered almost balancen because the number of 1s and 0s is almost the same (386 and 362, respectively). The ratio of 1s to 0s is 386/362 = 1.067."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### b) Pre-processing datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing(data):\n",
    "    stop_words = set(stopwords.words('english')) # find stop words in English language\n",
    "    lemmatizer = WordNetLemmatizer() # declare nltk lemmatizer\n",
    "\n",
    "    # iterate through every sentence and replace it by itself lemmatized, without punctuation and without stop words\n",
    "    for i in data['Sentence'].index:\n",
    "    \n",
    "        # remove punctuation\n",
    "        sentence_no_punct = ''\n",
    "        for char in (data.at[i, 'Sentence']):\n",
    "            if char not in string.punctuation:\n",
    "                sentence_no_punct = sentence_no_punct + char\n",
    "        (data.at[i, 'Sentence']) = sentence_no_punct\n",
    "\n",
    "        word_tokens = word_tokenize(data.at[i, 'Sentence'])\n",
    "        # remove stop words and lemmatize\n",
    "        word_tokens = [lemmatizer.lemmatize(word) for word in word_tokens if word not in stop_words]\n",
    "        word_tokens = [lemmatizer.lemmatize(word, 'v') for word in word_tokens]\n",
    "        word_tokens = [lemmatizer.lemmatize(word, 'a') for word in word_tokens]\n",
    "        # remove conjunction words\n",
    "        word_tokens = [word for word in word_tokens if word[-2:] != 'nt']\n",
    "        (data.at[i, 'Sentence']) = ' '.join(word_tokens)\n",
    "        \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert all letters to lower case\n",
    "yelp = yelp.applymap(lambda s:s.lower() if type(s) == str else s)\n",
    "imdb = imdb.applymap(lambda s:s.lower() if type(s) == str else s)\n",
    "amazon = amazon.applymap(lambda s:s.lower() if type(s) == str else s)\n",
    "\n",
    "# lemmatize, remove punctuation, remove stop words\n",
    "preprocessing(yelp)\n",
    "preprocessing(imdb)\n",
    "preprocessing(amazon);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this part, we decided to convert all sentences to lower case, so that the same word with some upper case letters and without them would not be detected as different words since we are using the string type which takes into account their differences. \n",
    "\n",
    "We also stripped the sentences of stop words because they do not add any meaning as the same stop words appear in many different sentences.\n",
    "\n",
    "Additionally, we removed the punctuation because it does not add any meaning to the word analysis exercise we will do in this question. \n",
    "\n",
    "We also lemmatized all the words because we are interested in knowing which class of words they belong to in order to understand the meaning of the sentence and not whether they are a noun, adjective, etc.\n",
    "\n",
    "We finally removed words with the conjunction \"n't\" because these are also meaningless words like \"isn't\" and \"didn't\". They are basically stop words but are not caught by the stop word checker because of the extra \"n't\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### c) Split training and testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the three datasets into training and testing data according to the specifications\n",
    "\n",
    "def split_data(data):\n",
    "    data.reset_index(drop=True)\n",
    "    training = (data.query('Label == 1' )).head(400).append((data.query('Label == 0' )).head(400))\n",
    "    testing = (data.query('Label == 1' )).tail(100).append((data.query('Label == 0' )).tail(100))\n",
    "    return training, testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split yelp\n",
    "training_yelp, testing_yelp = split_data(yelp)\n",
    "\n",
    "# split imdb\n",
    "training_imdb, testing_imdb = split_data(imdb)\n",
    "\n",
    "# split amazon\n",
    "training_amazon, testing_amazon = split_data(amazon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2348, 2)\n",
      "(600, 2)\n"
     ]
    }
   ],
   "source": [
    "# concatenate training and testing data for all files\n",
    "train_data = training_yelp.append(training_imdb, ignore_index=True).append(training_amazon, ignore_index=True)\n",
    "print(np.shape(train_data)) # this not exactly 2400? Because IMDB does not reach 800 data points...but why\n",
    "\n",
    "test_data = testing_yelp.append(testing_imdb, ignore_index=True).append(testing_amazon, ignore_index=True)\n",
    "print(np.shape(test_data)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### d) Bag of Words model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this question we cannot use the testing set to create the dictionary of unique words because the model needs to be created with the training set so that we can use the testing set as new data to test our model's ability to generalize. If we create the dictionary with the testing data, we are esentially using all the data as training data and would need to look for another set of new data to test the classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create set of unique words in training set\n",
    "word_dictionary = {}\n",
    "\n",
    "# iterate through every word or every sentence and store it in dictionary with count 0 (the count will be updated\n",
    "# later when we iterate through both testing and training set\n",
    "for i in train_data.index:\n",
    "    word_tokens_training = word_tokenize(train_data.at[i, 'Sentence'])\n",
    "    for word in word_tokens_training:\n",
    "        if word not in word_dictionary.keys():\n",
    "            word_dictionary[word] = 0\n",
    "# count the number of occurences of each word in dictionary in training set\n",
    "for i in train_data.index:\n",
    "    word_tokens_training = word_tokenize(train_data.at[i, 'Sentence'])\n",
    "    for word in word_tokens_training:\n",
    "        if word in word_dictionary.keys():\n",
    "            word_dictionary[word] += 1\n",
    "            \n",
    "# count the number of occurences of each word in dictionary in testing set  \n",
    "for i in test_data.index:\n",
    "    word_tokens_testing = word_tokenize(test_data.at[i, 'Sentence'])\n",
    "    for word in word_tokens_testing:\n",
    "        if word in word_dictionary.keys():\n",
    "            word_dictionary[word] += 1\n",
    "# create one feature vector per review\n",
    "feature_column = [] # list to store the feature vectors and add to dataframe at the end\n",
    "j = 0; # to indicate position in iterating through feature vector\n",
    "\n",
    "for i in train_data.index:\n",
    "    word_tokens_training = word_tokenize(train_data.at[i, 'Sentence'])\n",
    "    feature_vector = [0] * len(word_dictionary.keys()) # to store feature vector in each iteration\n",
    "    for dict_word in word_dictionary.keys():\n",
    "        if dict_word in word_tokens_training:\n",
    "            feature_vector[j] = math.log(word_dictionary[dict_word] + 1) # apply log normalization\n",
    "            j += 1\n",
    "    feature_column.append(feature_vector) \n",
    "    j = 0\n",
    "    \n",
    "train_data['Vectors'] = feature_column"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "e) Pick postprocessing strategy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will not apply mean substraction and division by standard deviation because this only makes sense if the data more or less follows a normal distribution. Since we cannot ensure this, we will choose some other method.\n",
    "\n",
    "Also, the number of dimensions (words) of the feature vectors is significant, so choosing L1 or L2 norm might result in similar distance measures for sentences with different words, so we discarded these techniques. \n",
    "\n",
    "Log normalization can allow us to make data less skewed and focus on the relative differences rather than the absolute values, so we chose to apply this technique.\n",
    "\n",
    "We added the normalization step in the code for d) as we were constructing the feature vectors in order to reduce the number of iterations we need to do through the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-46-1d50046d236c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[0mvector\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_data\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mat\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'Vectors'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0melement_index\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvector\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m         \u001b[0mlog_norm\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvector\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0melement_index\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m         \u001b[1;33m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mat\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'Vectors'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0melement_index\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlog_norm\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# we can ignore this because it takes too long and I added the normalization in d) - not deleting it just in case\n",
    "\n",
    "# log normalization implementation\n",
    "\n",
    "log_norm = 0; # variable to store log norm value each iteration\n",
    "\n",
    "for i in train_data['Vectors'].index: # loop to substitute each feature vector by its log normalization \n",
    "    vector = train_data.at[i, 'Vectors']\n",
    "    for element_index in range(len(vector)):\n",
    "        log_norm = math.log(vector[element_index] + 1)\n",
    "        (train_data.at[i, 'Vectors'])[element_index] = log_norm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "f) Sentiment prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "setting an array element with a sequence.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-64-bc31d67972de>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[1;31m# initialize and fit naive bayes model gaussian prior with training data\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[0mgnb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mGaussianNB\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 16\u001b[1;33m \u001b[0mgnb\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\naive_bayes.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    187\u001b[0m         \u001b[0mself\u001b[0m \u001b[1;33m:\u001b[0m \u001b[0mobject\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    188\u001b[0m         \"\"\"\n\u001b[1;32m--> 189\u001b[1;33m         \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcheck_X_y\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    190\u001b[0m         return self._partial_fit(X, y, np.unique(y), _refit=True,\n\u001b[0;32m    191\u001b[0m                                  sample_weight=sample_weight)\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36mcheck_X_y\u001b[1;34m(X, y, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, warn_on_dtype, estimator)\u001b[0m\n\u001b[0;32m    717\u001b[0m                     \u001b[0mensure_min_features\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mensure_min_features\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    718\u001b[0m                     \u001b[0mwarn_on_dtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mwarn_on_dtype\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 719\u001b[1;33m                     estimator=estimator)\n\u001b[0m\u001b[0;32m    720\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mmulti_output\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    721\u001b[0m         y = check_array(y, 'csr', force_all_finite=True, ensure_2d=False,\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[1;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, warn_on_dtype, estimator)\u001b[0m\n\u001b[0;32m    494\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    495\u001b[0m                 \u001b[0mwarnings\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msimplefilter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'error'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mComplexWarning\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 496\u001b[1;33m                 \u001b[0marray\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0morder\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0morder\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    497\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mComplexWarning\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    498\u001b[0m                 raise ValueError(\"Complex data not supported\\n\"\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\numpy\\core\\numeric.py\u001b[0m in \u001b[0;36masarray\u001b[1;34m(a, dtype, order)\u001b[0m\n\u001b[0;32m    536\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    537\u001b[0m     \"\"\"\n\u001b[1;32m--> 538\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0morder\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0morder\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    539\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    540\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\series.py\u001b[0m in \u001b[0;36m__array__\u001b[1;34m(self, dtype)\u001b[0m\n\u001b[0;32m    726\u001b[0m             \u001b[0mwarnings\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwarn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mFutureWarning\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstacklevel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    727\u001b[0m             \u001b[0mdtype\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'M8[ns]'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 728\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    729\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    730\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__array_wrap__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcontext\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\numpy\\core\\numeric.py\u001b[0m in \u001b[0;36masarray\u001b[1;34m(a, dtype, order)\u001b[0m\n\u001b[0;32m    536\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    537\u001b[0m     \"\"\"\n\u001b[1;32m--> 538\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0morder\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0morder\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    539\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    540\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\arrays\\numpy_.py\u001b[0m in \u001b[0;36m__array__\u001b[1;34m(self, dtype)\u001b[0m\n\u001b[0;32m    169\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    170\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__array__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 171\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_ndarray\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    172\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    173\u001b[0m     \u001b[0m_HANDLED_TYPES\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnumbers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mNumber\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\numpy\\core\\numeric.py\u001b[0m in \u001b[0;36masarray\u001b[1;34m(a, dtype, order)\u001b[0m\n\u001b[0;32m    536\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    537\u001b[0m     \"\"\"\n\u001b[1;32m--> 538\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0morder\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0morder\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    539\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    540\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: setting an array element with a sequence."
     ]
    }
   ],
   "source": [
    "# split training and testing data into X (data) and Y (labels)\n",
    "x_train = train_data['Vectors'] #something is wrong with these (dimensions or array type or wrong column) - working on it!\n",
    "y_train = train_data['Label']\n",
    "\n",
    "#x_test = test_data['Vectors'] # we still do not have vectors\n",
    "#y_test = test_data['Label']\n",
    "\n",
    "# initialize and fit logistic regression model with training data\n",
    "lr = LogisticRegression()\n",
    "lr.fit(x_train, y_train)\n",
    "\n",
    "# predict the result for the testing data\n",
    "lr_pred = lr.predict(x_test) \n",
    "\n",
    "# calculate accuracy\n",
    "lr_acc = metrics.accuracy_score(lr_pred, y_test)\n",
    "\n",
    "# initialize and fit naive bayes model gaussian prior with training data\n",
    "gnb = GaussianNB()\n",
    "gnb.fit(x_train, y_train)\n",
    "\n",
    "# predict the results for the test set\n",
    "gnb_pred = gnb.predict(x_test)\n",
    "\n",
    "# predict the result for the testing data\n",
    "gnb_pred = gnb.predict(x_test) \n",
    "\n",
    "# calculate accuracy\n",
    "gnb_acc = metrics.accuracy_score(gnb_pred, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       [1.9459101490553132, 4.532599493153256, 4.8520...\n",
       "1       [4.532599493153256, 2.0794415416798357, 2.6390...\n",
       "2       [2.6390573296152584, 2.8903717578961645, 5.389...\n",
       "3       [5.389071729816501, 2.8903717578961645, 0, 0, ...\n",
       "4       [5.389071729816501, 2.5649493574615367, 0, 0, ...\n",
       "5       [4.68213122712422, 1.3862943611198906, 0, 0, 0...\n",
       "6       [3.828641396489095, 0.6931471805599453, 0.6931...\n",
       "7       [3.295836866004329, 4.23410650459726, 0, 0, 0,...\n",
       "8       [4.836281906951478, 3.7376696182833684, 0, 0, ...\n",
       "9       [4.68213122712422, 4.3694478524670215, 1.60943...\n",
       "10      [4.07753744390572, 2.6390573296152584, 2.48490...\n",
       "11      [1.3862943611198906, 0, 0, 0, 0, 0, 0, 0, 0, 0...\n",
       "12      [2.9444389791664403, 0.6931471805599453, 1.609...\n",
       "13      [5.389071729816501, 2.5649493574615367, 2.3978...\n",
       "14      [2.8903717578961645, 4.3694478524670215, 1.098...\n",
       "15      [4.852030263919617, 4.07753744390572, 3.891820...\n",
       "16      [4.852030263919617, 5.081404364984463, 2.83321...\n",
       "17      [1.6094379124341003, 4.219507705176107, 1.3862...\n",
       "18      [4.007333185232471, 5.69035945432406, 1.098612...\n",
       "19      [3.871201010907891, 1.791759469228055, 0.69314...\n",
       "20      [1.9459101490553132, 2.1972245773362196, 1.386...\n",
       "21      [5.389071729816501, 4.68213122712422, 1.098612...\n",
       "22      [5.081404364984463, 4.07753744390572, 1.098612...\n",
       "23      [5.69035945432406, 2.833213344056216, 1.791759...\n",
       "24      [2.3978952727983707, 4.867534450455582, 0.6931...\n",
       "25      [2.8903717578961645, 1.3862943611198906, 2.890...\n",
       "26      [2.1972245773362196, 3.6375861597263857, 1.386...\n",
       "27      [4.343805421853684, 2.5649493574615367, 3.3672...\n",
       "28      [5.69035945432406, 1.0986122886681098, 0.69314...\n",
       "29      [2.0794415416798357, 2.5649493574615367, 0, 0,...\n",
       "                              ...                        \n",
       "2318    [4.477336814478207, 4.990432586778736, 2.19722...\n",
       "2319    [3.6375861597263857, 4.990432586778736, 0.6931...\n",
       "2320    [3.6635616461296463, 4.290459441148391, 3.0445...\n",
       "2321    [3.5553480614894135, 2.302585092994046, 2.8903...\n",
       "2322    [5.69035945432406, 3.6375861597263857, 4.70953...\n",
       "2323    [3.871201010907891, 4.867534450455582, 3.29583...\n",
       "2324    [5.081404364984463, 3.091042453358316, 3.04452...\n",
       "2325    [4.890349128221754, 3.1780538303479458, 3.1354...\n",
       "2326    [2.6390573296152584, 4.890349128221754, 3.8066...\n",
       "2327    [4.919980925828125, 1.6094379124341003, 3.0910...\n",
       "2328    [4.07753744390572, 5.69035945432406, 4.0775374...\n",
       "2329    [3.1354942159291497, 2.995732273553991, 2.7080...\n",
       "2330    [3.1354942159291497, 0, 0, 0, 0, 0, 0, 0, 0, 0...\n",
       "2331    [4.394449154672439, 4.477336814478207, 2.83321...\n",
       "2332    [3.871201010907891, 5.043425116919247, 1.60943...\n",
       "2333    [3.58351893845611, 2.5649493574615367, 4.99043...\n",
       "2334    [4.51085950651685, 1.9459101490553132, 1.94591...\n",
       "2335    [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...\n",
       "2336    [1.9459101490553132, 3.1780538303479458, 4.990...\n",
       "2337    [1.6094379124341003, 1.791759469228055, 0, 0, ...\n",
       "2338    [5.389071729816501, 3.828641396489095, 3.87120...\n",
       "2339    [2.772588722239781, 1.791759469228055, 2.07944...\n",
       "2340    [4.990432586778736, 0, 0, 0, 0, 0, 0, 0, 0, 0,...\n",
       "2341    [5.69035945432406, 2.772588722239781, 3.871201...\n",
       "2342    [2.9444389791664403, 1.6094379124341003, 0.693...\n",
       "2343    [3.4965075614664802, 3.871201010907891, 0, 0, ...\n",
       "2344    [4.709530201312334, 2.772588722239781, 2.63905...\n",
       "2345    [3.1354942159291497, 0, 0, 0, 0, 0, 0, 0, 0, 0...\n",
       "2346    [4.51085950651685, 1.0986122886681098, 1.60943...\n",
       "2347    [5.69035945432406, 1.6094379124341003, 0, 0, 0...\n",
       "Name: Vectors, Length: 2348, dtype: object"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
